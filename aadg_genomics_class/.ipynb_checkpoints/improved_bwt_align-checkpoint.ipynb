{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b601b596-d753-4623-8c70-defb0a39c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoked CLI with the following args: /Users/styczynski/Library/Caches/pypoetry/virtualenvs/aadg-genomics-class-g6v9BdZu-py3.11/lib/python3.11/site-packages/ipykernel_launcher.py -f /Users/styczynski/Library/Jupyter/runtime/kernel-038f444b-18f2-47cb-b801-04020e02259a.json\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=1000020\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=2000040\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=3000060\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=4000080\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=5000100\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=6000120\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=7000140\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=8000160\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=9000180\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=10000200\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=11000220\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=12000240\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=13000260\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=14000280\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=15000300\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=16000320\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=17000340\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=18000360\n",
      "PROCESS CHUNK 1000020\n",
      "PROCESSED ENTIRE CHUNK! offset=19000380\n",
      "PROCESS CHUNK 999620\n",
      "PROCESSED ENTIRE CHUNK! offset=20000000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from typing import Dict, Any, Set, Optional, Tuple\n",
    "from itertools import chain\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "GLO_Q = None\n",
    "GLO_T = None\n",
    "GLO_CHUNK = None\n",
    "\n",
    "MASKS: Optional[Dict[int, int]] = None\n",
    "MAX_KMER_SIZE = 64\n",
    "\n",
    "MAPPING_DOLLAR = 0\n",
    "MAPPING = dict(\n",
    "    C=1,\n",
    "    A=2,\n",
    "    T=3,\n",
    "    G=4,\n",
    "    #a=1,\n",
    "    #c=0,\n",
    "    #g=3,\n",
    "    #G=4,\n",
    "    #t=2,\n",
    "    #T=3,\n",
    ")\n",
    "\n",
    "ALPHABET = set(MAPPING.values())\n",
    "ALPHABET_DOLLAR = set([*ALPHABET, MAPPING_DOLLAR])\n",
    "\n",
    "RR_MAPPING = [\"C\", \"A\", \"T\", \"G\"]\n",
    "\n",
    "COMPLEMENT_MAPPING = {\n",
    "    1: 2,\n",
    "    0: 3,\n",
    "    3: 0,\n",
    "    2: 1,\n",
    "}\n",
    "\n",
    "MAPPING_FN = np.vectorize(MAPPING.get, otypes=[np.uint8])\n",
    "COMPLEMENT_MAPPING_FN = np.vectorize(COMPLEMENT_MAPPING.get)\n",
    "\n",
    "# DP algorithm adapted from Langmead's notebooks\n",
    "def align_dp_trace(D, x, y):\n",
    "    ''' Backtrace edit-distance matrix D for strings x and y '''\n",
    "    i, j = len(x), len(y)\n",
    "    while i > 0:\n",
    "        diag, vert, horz = sys.maxsize, sys.maxsize, sys.maxsize\n",
    "        delt = None\n",
    "        if i > 0 and j > 0:\n",
    "            delt = 0 if x[i-1] == y[j-1] else 1\n",
    "            diag = D[i-1, j-1] + delt\n",
    "        if i > 0:\n",
    "            vert = D[i-1, j] + 1\n",
    "        if j > 0:\n",
    "            horz = D[i, j-1] + 1\n",
    "        if diag <= vert and diag <= horz:\n",
    "            # diagonal was best\n",
    "            i -= 1; j -= 1\n",
    "        elif vert <= horz:\n",
    "            # vertical was best; this is an insertion in x w/r/t y\n",
    "            i -= 1\n",
    "        else:\n",
    "            # horizontal was best\n",
    "            j -= 1\n",
    "    # j = offset of the first (leftmost) character of t involved in the\n",
    "    # alignment\n",
    "    return j\n",
    "\n",
    "def align_dp_k_edit(p, t):\n",
    "    ''' Find the alignment of p to a substring of t with the fewest edits.  \n",
    "        Return the edit distance and the coordinates of the substring. '''\n",
    "    D = np.zeros((len(p)+1, len(t)+1), dtype=int)\n",
    "    # Note: First row gets zeros.  First column initialized as usual.\n",
    "    D[1:, 0] = range(1, len(p)+1)\n",
    "    for i in range(1, len(p)+1):\n",
    "        for j in range(1, len(t)+1):\n",
    "            delt = 1 if p[i-1] != t[j-1] else 0\n",
    "            D[i, j] = min(D[i-1, j-1] + delt, D[i-1, j] + 1, D[i, j-1] + 1)\n",
    "    # Find minimum edit distance in last row\n",
    "    mnJ, mn = None, len(p) + len(t)\n",
    "    for j in range(len(t)+1):\n",
    "        if D[len(p), j] < mn:\n",
    "            mnJ, mn = j, D[len(p), j]\n",
    "    # Backtrace; note: stops as soon as it gets to first row\n",
    "    off = align_dp_trace(D, p, t[:mnJ])\n",
    "    # Return edit distance and t coordinates of aligned substring\n",
    "    return mn, off, mnJ\n",
    "\n",
    "\n",
    "def run_match_align_dp(target, query, align_mode=1):\n",
    "    for (k, step) in [(15, 11), (10, 11), (8, 5)]:\n",
    "        suff_len_factor = 0.4\n",
    "        suff_len_factor2 = 0.6\n",
    "\n",
    "        print(f\"align_seq(): ALIGNER_MODE {align_mode}\")\n",
    "        suff_len = round(len(query) * suff_len_factor)\n",
    "        suff_len2 = round(len(query) * suff_len_factor2)\n",
    "\n",
    "        target_suffix = target[suff_len-k:]\n",
    "        query_suffix = query[(len(query)-suff_len):]\n",
    "\n",
    "        edist = len(query)//9\n",
    "        kmers = defaultdict(list)\n",
    "\n",
    "        mask = generate_mask(k)\n",
    "\n",
    "        uadd = np.frompyfunc(lambda x, y: ((x << 2) | y) & mask, 2, 1)\n",
    "\n",
    "        # This computes values for kmers\n",
    "        kmers_target = uadd.accumulate(target_suffix, dtype=object).astype(int)\n",
    "        # for i in range(0, len(target)-k+1, step):\n",
    "        #         kmers[np.sum(target[i:i+k])].append(i)\n",
    "        for i in range(0, len(kmers_target), step):\n",
    "            kmers[kmers_target[i]].append(i)\n",
    "\n",
    "        hits = []\n",
    "        kmers_query = uadd.accumulate(query_suffix, dtype=object).astype(int)\n",
    "        #for i in range(0, len(query)-k+1, step+1):\n",
    "            #for j in kmers[np.sum(query[i:i+k])]:\n",
    "        for i in range(0, len(kmers_query), step+1):\n",
    "            for j in kmers[kmers_query[i]]:\n",
    "                lf = max(0, j-i-edist)\n",
    "                rt = min(len(target_suffix), j-i+len(query_suffix)+edist)\n",
    "                mn, soff, eoff = align_dp_k_edit(query_suffix, target_suffix[lf:rt])\n",
    "                soff += lf\n",
    "                eoff += lf\n",
    "                if mn <= edist:\n",
    "                    hits.append((mn, soff, eoff))\n",
    "        hits.sort()\n",
    "        if hits:\n",
    "            return 0, len(target_suffix)-hits[0][2]\n",
    "        if align_mode == 1:\n",
    "            return 0, 0\n",
    "    return 0, 0\n",
    "\n",
    "def merge(x: np.array, SA12: np.array, SA3: np.array) -> np.array:\n",
    "    \"Merge the suffixes in sorted SA12 and SA3.\"\n",
    "    ISA = np.zeros((len(x),), dtype='int')\n",
    "    for i in range(len(SA12)):\n",
    "        ISA[SA12[i]] = i\n",
    "    SA = np.zeros((len(x),), dtype='int')\n",
    "    idx = 0\n",
    "    i, j = 0, 0\n",
    "    while i < len(SA12) and j < len(SA3):\n",
    "        if less(x, SA12[i], SA3[j], ISA):\n",
    "            SA[idx] = SA12[i]\n",
    "            idx += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            SA[idx] = SA3[j]\n",
    "            idx += 1\n",
    "            j += 1\n",
    "    if i < len(SA12):\n",
    "        SA[idx:len(SA)] = SA12[i:]\n",
    "    elif j < len(SA3):\n",
    "        SA[idx:len(SA)] = SA3[j:]\n",
    "    return SA\n",
    "\n",
    "\n",
    "def u_idx(i: int, m: int) -> int:\n",
    "    \"Map indices in u back to indices in the original string.\"\n",
    "    if i < m:\n",
    "        return 1 + 3 * i\n",
    "    else:\n",
    "        return 2 + 3 * (i - m - 1)\n",
    "\n",
    "\n",
    "def safe_idx(x: np.array, i: int) -> int:\n",
    "    \"Hack to get zero if we index beyond the end.\"\n",
    "    return 0 if i >= len(x) else x[i]\n",
    "\n",
    "def symbcount(x: np.array, asize: int) -> np.array:\n",
    "    \"Count how often we see each character in the alphabet.\"\n",
    "    counts = np.zeros((asize,), dtype=\"int\")\n",
    "    for c in x:\n",
    "        counts[c] += 1\n",
    "    return counts\n",
    "\n",
    "def cumsum(counts: np.array) -> np.array:\n",
    "    \"Compute the cumulative sum from the character count.\"\n",
    "    res = np.zeros((len(counts, )), dtype='int')\n",
    "    acc = 0\n",
    "    for i, k in enumerate(counts):\n",
    "        res[i] = acc\n",
    "        acc += k\n",
    "    return res\n",
    "\n",
    "def bucket_sort(x: np.array, asize: int,\n",
    "                idx: np.array, offset: int = 0) -> np.array:\n",
    "    \"Sort indices in idx according to x[i + offset].\"\n",
    "    sort_symbs = np.array([safe_idx(x, i + offset) for i in idx])\n",
    "    counts = symbcount(sort_symbs, asize)\n",
    "    buckets = cumsum(counts)\n",
    "    out = np.zeros((len(idx),), dtype='int')\n",
    "    for i in idx:\n",
    "        bucket = safe_idx(x, i + offset)\n",
    "        out[buckets[bucket]] = i\n",
    "        buckets[bucket] += 1\n",
    "    return out\n",
    "\n",
    "def radix3(x: np.array, asize: int, idx: np.array) -> np.array:\n",
    "    \"Sort indices in idx according to their first three letters in x.\"\n",
    "    idx = bucket_sort(x, asize, idx, 2)\n",
    "    idx = bucket_sort(x, asize, idx, 1)\n",
    "    return bucket_sort(x, asize, idx)\n",
    "\n",
    "def triplet(x: np.array, i: int) -> Tuple[int, int, int]:\n",
    "    \"Extract the triplet (x[i],x[i+1],x[i+2]).\"\n",
    "    return safe_idx(x, i), safe_idx(x, i + 1), safe_idx(x, i + 2)\n",
    "\n",
    "def collect_alphabet(x: np.array, idx: np.array) -> Tuple[np.array, int]:\n",
    "    \"Map the triplets starting at idx to a new alphabet.\"\n",
    "    alpha = np.zeros((len(x),), dtype='int')\n",
    "    value = 1\n",
    "    last_trip = -1, -1, -1\n",
    "    for i in idx:\n",
    "        trip = triplet(x, i)\n",
    "        if trip != last_trip:\n",
    "            value += 1\n",
    "            last_trip = trip\n",
    "        alpha[i] = value\n",
    "    return alpha, value - 1\n",
    "\n",
    "def build_u(x: np.array, alpha: np.array) -> np.array:\n",
    "    \"Construct u string, using 1 as central sentinel.\"\n",
    "    a = np.array([alpha[i] for i in range(1, len(x), 3)] +\n",
    "                 [1] +\n",
    "                 [alpha[i] for i in range(2, len(x), 3)])\n",
    "    return a\n",
    "\n",
    "def less(x: np.array, i: int, j: int, ISA: np.array) -> bool:\n",
    "    \"Check if x[i:] < x[j:] using the inverse suffix array for SA12.\"\n",
    "    a: int = safe_idx(x, i)\n",
    "    b: int = safe_idx(x, j)\n",
    "    if a < b: return True\n",
    "    if a > b: return False\n",
    "    if i % 3 != 0 and j % 3 != 0: return ISA[i] < ISA[j]\n",
    "    return less(x, i + 1, j + 1, ISA)\n",
    "\n",
    "def skew_rec(x: np.array, asize: int) -> np.array:\n",
    "    \"skew/DC3 SA construction algorithm.\"\n",
    "\n",
    "    SA12 = np.array([i for i in range(len(x)) if i % 3 != 0])\n",
    "\n",
    "    SA12 = radix3(x, asize, SA12)\n",
    "    new_alpha, new_asize = collect_alphabet(x, SA12)\n",
    "    if new_asize < len(SA12):\n",
    "        # Recursively sort SA12\n",
    "        u = build_u(x, new_alpha)\n",
    "        sa_u = skew_rec(u, new_asize + 2)\n",
    "        m = len(sa_u) // 2\n",
    "        SA12 = np.array([u_idx(i, m) for i in sa_u if i != m])\n",
    "\n",
    "    if len(x) % 3 == 1:\n",
    "        SA3 = np.array([len(x) - 1] + [i - 1 for i in SA12 if i % 3 == 1])\n",
    "    else:\n",
    "        SA3 = np.array([i - 1 for i in SA12 if i % 3 == 1])\n",
    "    SA3 = bucket_sort(x, asize, SA3)\n",
    "    return merge(x, SA12, SA3)\n",
    "\n",
    "# DUŻO SYFU\n",
    "C = {}\n",
    "O = {}\n",
    "D = []\n",
    "\n",
    "# rewards/penalties\n",
    "gap_open = 3\n",
    "gap_ext = 1\n",
    "mismatch = 1\n",
    "match = 0\n",
    "\n",
    "# option switches\n",
    "sub_mat = {}\n",
    "\n",
    "num_prunes = 0\n",
    "# insertion -> 1\n",
    "# delection -> 2\n",
    "# match -> 0\n",
    "# mismatch -> 3\n",
    "# start -> 4\n",
    "\n",
    "def compute_C(totals):\n",
    "    \"\"\"compute C, the number of lexicographically greater symbols in the ref\"\"\"\n",
    "    #C = {0: 0, 1: 0, 2: 0, 3: 0, MAPPING_DOLLAR: 0}\n",
    "    C = {v: 0 for v in ALPHABET_DOLLAR}\n",
    "    for k in ALPHABET:\n",
    "        for ref in ALPHABET:\n",
    "            if ref < k:\n",
    "                C[k] += totals[ref]\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def compute_D(s, C, Oprime, bw):\n",
    "    \"\"\"compute estimated lower bounds of differences in substring s[0:i] for all  in [0,len(s)]\"\"\"\n",
    "    k = 1\n",
    "    l = len(bw)-2\n",
    "    z = 0\n",
    "    D = [0] * len(s)\n",
    "\n",
    "    for i in range(0, len(s)):\n",
    "        k = C[s[i]] + Oprime[s[i]][k-1] + 1\n",
    "        l = C[s[i]] + Oprime[s[i]][l]\n",
    "        # k = C[s[i]] + 1\n",
    "        # l = C[s[i]]\n",
    "        if k > l:\n",
    "            k = 1\n",
    "            l = len(bw)-1\n",
    "            z += 1\n",
    "        D[i] = z\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "def get_D(i):\n",
    "    \"\"\"enforce condition that if D[i] is set to -1, its value will be considered as 0\"\"\"\n",
    "    if i < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return D[i]\n",
    "\n",
    "\n",
    "def get_O(char, index):\n",
    "    \"\"\"see get_D()\"\"\"\n",
    "    if index < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return O[char][index]\n",
    "\n",
    "\n",
    "def inexact_recursion(s, i, diff, k, l, prev_type):\n",
    "    \"\"\"search bwt recursively and tolerate errors\"\"\"\n",
    "    \n",
    "    global num_prunes\n",
    "\n",
    "    # pruning based on estimated mistakes\n",
    "    if diff < get_D(i):\n",
    "        num_prunes += 1\n",
    "        return set()\n",
    "\n",
    "    # end of query condition\n",
    "    temp = set()\n",
    "    if i < 0:\n",
    "        for j in range(k, l+1):\n",
    "            temp.add((j, diff))\n",
    "        return temp\n",
    "\n",
    "    # search\n",
    "    sa_idx = set()  # set of suffix array indices at which a match starts\n",
    "    \n",
    "    # Insertion\n",
    "    if prev_type == 1:\n",
    "        sa_idx = sa_idx.union(inexact_recursion(s, i-1, diff-gap_ext, k, l, 1))\n",
    "    else:\n",
    "        sa_idx = sa_idx.union(inexact_recursion(s, i-1, diff-gap_ext-gap_open, k, l, 1))\n",
    "\n",
    "    for char in ALPHABET:\n",
    "        temp_k = C[char] + get_O(char, k-1) + 1\n",
    "        temp_l = C[char] + get_O(char, l)\n",
    "    \n",
    "        if temp_k <= temp_l:\n",
    "            # Deletion\n",
    "            if prev_type == 2:\n",
    "                sa_idx = sa_idx.union(inexact_recursion(s, i, diff-gap_ext, temp_k, temp_l, 2))\n",
    "            else:\n",
    "                sa_idx = sa_idx.union(inexact_recursion(s, i, diff-gap_ext-gap_open, temp_k, temp_l, 2))\n",
    "            if char == s[i]:\n",
    "                # Match!\n",
    "                sa_idx = sa_idx.union(inexact_recursion(s, i-1, diff+match, temp_k, temp_l, 0))\n",
    "                \n",
    "            else:\n",
    "                # Mismatch\n",
    "                if sub_mat:\n",
    "                    sa_idx = sa_idx.union(inexact_recursion(s, i-1, diff-mismatch*sub_mat[(s[i], char)],\n",
    "                                                            temp_k, temp_l, 3))\n",
    "                else:\n",
    "                    sa_idx = sa_idx.union(inexact_recursion(s, i-1, diff-mismatch, temp_k, temp_l, 3))\n",
    "\n",
    "    return sa_idx\n",
    "\n",
    "\n",
    "def estimate_substitution_mat(ref, r):\n",
    "    \"\"\"get likelihood of each substitution type over all possible alignments\"\"\"\n",
    "    mismatches = {}\n",
    "\n",
    "    for i in range(0, len(ref)):\n",
    "        for j in range(0, len(r)):\n",
    "            if ref[i] != r[j]:\n",
    "                if (ref[i], r[j]) in mismatches:\n",
    "                    mismatches[(ref[i], r[j])] += 1\n",
    "                else:\n",
    "                    mismatches[(ref[i], r[j])] = 1\n",
    "\n",
    "    scale = max(mismatches.values())\n",
    "    for k in mismatches:\n",
    "        mismatches[k] = float(mismatches[k])/scale\n",
    "\n",
    "    return mismatches\n",
    "\n",
    "def rank(bw):\n",
    "    \"\"\"rank(char) := list of number of occurrences of a char for each substring R[:i] (reference)\"\"\"\n",
    "    totals = {}\n",
    "    ranks = {}\n",
    "\n",
    "    for char in ALPHABET:\n",
    "        if (char not in totals) and (char != MAPPING_DOLLAR): # '$':\n",
    "            totals[char] = 0\n",
    "            ranks[char] = []\n",
    "\n",
    "    for char in bw:\n",
    "        if char != MAPPING_DOLLAR: # '$':\n",
    "            totals[char] += 1\n",
    "        for t in totals.keys():\n",
    "            ranks[t].append(totals[t])\n",
    "\n",
    "    return ranks, totals\n",
    "\n",
    "def inexact_search(bw, bwr, s, diff):\n",
    "    \"\"\"find suffix array intervals with up to diff differences\"\"\"\n",
    "\n",
    "    global C, O, D, num_prunes\n",
    "    # totals, ranks\n",
    "    # O is a dictionary with keys $,A,C,G,T, and values are arrays of counts\n",
    "    O, tot = rank(bw)\n",
    "\n",
    "    # reverse ranks\n",
    "    Oprime, junk = rank(bwr)\n",
    "    #Oprime = None\n",
    "\n",
    "    # C[a] := number of lexicographically smaller letters than a in bw/reference\n",
    "    C = compute_C(tot)\n",
    "\n",
    "    # D[i] := lower bound on number of differences in substring s[1:i]\n",
    "    D = compute_D(s, C, Oprime, bw)\n",
    "\n",
    "    # call the recursive search function and return a list of SA-range tuples\n",
    "    sa_index_set = inexact_recursion(s, len(s)-1, diff, 0, len(bw)-1, 4)\n",
    "    index_dict = {}\n",
    "\n",
    "    for (i, j) in sa_index_set:\n",
    "        # if index already exists, pick the higher diff value\n",
    "        if i in index_dict:\n",
    "            if index_dict[i] < j:\n",
    "                index_dict[i] = j\n",
    "                num_prunes += 1\n",
    "\n",
    "        else:\n",
    "            index_dict[i] = j\n",
    "\n",
    "    # sort list by diff from highest to lowest\n",
    "    return sorted(index_dict.items(), key=itemgetter(1), reverse=True) \n",
    "\n",
    "\n",
    "def best_match_position(bw, bwr, s, diff, sa):\n",
    "    sa_index_list = inexact_search(bw, bwr, s, diff)\n",
    "    if len(sa_index_list) != 0:\n",
    "        best_index, score = sa_index_list[0]\n",
    "        return sa[best_index]+1, score\n",
    "    else:\n",
    "        return -1, -1\n",
    "\n",
    "def normalize_pos(pos, len):\n",
    "    return min(max(pos, 0), len)\n",
    "\n",
    "\n",
    "def generate_mask(\n",
    "    kmer_len: int,\n",
    ") -> int:\n",
    "    global MASKS\n",
    "    if not MASKS:\n",
    "        MASKS = dict()\n",
    "        ret = 3\n",
    "        for i in range(MAX_KMER_SIZE+1):\n",
    "            ret = (ret << 2) | 3\n",
    "            MASKS[i] = ret\n",
    "    return MASKS[kmer_len]\n",
    "\n",
    "\n",
    "def get_minimizers(\n",
    "    seq_arr,\n",
    "    kmer_len,\n",
    "    window_len,\n",
    "):\n",
    "    sequence_len = len(seq_arr)\n",
    "    mask = generate_mask(kmer_len)\n",
    "\n",
    "    # Function to compute kmer value based on the previous (on the left side) kmer value and new nucleotide\n",
    "    uadd = np.frompyfunc(lambda x, y: ((x << 2) | y) & mask, 2, 1)\n",
    "\n",
    "    # This computes values for kmers\n",
    "    kmers = uadd.accumulate(seq_arr, dtype=object).astype(int)\n",
    "    kmers[:kmer_len-2] = 0\n",
    "    del seq_arr\n",
    "    \n",
    "    # Do sliding window and get min kmers positions\n",
    "    kmers_min_pos = np.add(np.argmin(sliding_window_view(kmers, window_shape=window_len), axis=1), np.arange(0, sequence_len - window_len + 1))\n",
    "    \n",
    "    # Now collect all selected mimumum and kmers into single table\n",
    "    selected_kmers = np.column_stack((\n",
    "        kmers[kmers_min_pos],\n",
    "        kmers_min_pos,\n",
    "        #np.ones(len(kmers_min_pos), dtype=bool)\n",
    "    ))[kmer_len:]\n",
    "    del kmers_min_pos\n",
    "    del kmers\n",
    "\n",
    "    # Remove duplicates\n",
    "    selected_kmers = selected_kmers[selected_kmers[:, 0].argsort()]\n",
    "    selected_kmers = np.unique(selected_kmers, axis=0)\n",
    "\n",
    "    # This part performs group by using the kmer value\n",
    "    selected_kmers_unique_idx = np.unique(selected_kmers[:, 0], return_index=True)[1][1:]\n",
    "    selected_kmers_entries_split = np.split(selected_kmers[:, 1], selected_kmers_unique_idx)\n",
    "\n",
    "    if len(selected_kmers) > 0:\n",
    "        # We zip all kmers into a dict\n",
    "        result = dict(zip(chain([selected_kmers[0, 0]], selected_kmers[selected_kmers_unique_idx, 0]), selected_kmers_entries_split))\n",
    "    else:\n",
    "        # If we have no minimizers we return nothing, sorry\n",
    "        result = dict()\n",
    "    return result\n",
    "\n",
    "\n",
    "def cartesian_product(*arrays):\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)\n",
    "\n",
    "def run_aligner_pipeline(\n",
    "    reference_file_path: str,\n",
    "    reads_file_path: str,\n",
    "    output_file_path: str,\n",
    "    kmer_len: int,\n",
    "    window_len: int,\n",
    "):\n",
    "    global GLO_Q\n",
    "    global GLO_T\n",
    "    global GLO_CHUNK\n",
    "    \n",
    "    gc.disable()\n",
    "    #tracemalloc.start()\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    print(f\"Invoked CLI with the following args: {' '.join(sys.argv)}\")\n",
    "    \n",
    "    expected_coords = {}\n",
    "    with open('../data_big/reads20Mb.txt', mode ='r')as file:\n",
    "        csvFile = csv.reader(file, delimiter='\\t')\n",
    "        expected_coords = {line[0]: (int(line[1]), int(line[2])) for line in csvFile}\n",
    "    if kmer_len > MAX_KMER_SIZE:\n",
    "        kmer_len = MAX_KMER_SIZE\n",
    "\n",
    "    target_seq = None\n",
    "    ref_loaded = False\n",
    "    all_seq = \"\"\n",
    "    all_seq_len = 0\n",
    "    index_offset = 0\n",
    "    CHUNK_SIZE = 1000000 # Chunk size should be around 1000000\n",
    "\n",
    "    ref_index = dict()\n",
    "    with open(reference_file_path) as ref_fh:\n",
    "        for line in chain(ref_fh, [\">\"]):\n",
    "            if line[0] != '>':\n",
    "                fasta_line = line.rstrip()\n",
    "                all_seq += fasta_line\n",
    "                all_seq_len  += len(fasta_line)\n",
    "            if (all_seq_len >= CHUNK_SIZE or line[0] == '>') and all_seq_len > 0:\n",
    "                print(f\"PROCESS CHUNK {all_seq_len}\")\n",
    "                \n",
    "                # all_seq_len = 0\n",
    "                # all_seq = 0\n",
    "\n",
    "                seq_arr = MAPPING_FN(np.array(list(all_seq)))\n",
    "                GLO_CHUNK = seq_arr\n",
    "                if target_seq is None:\n",
    "                   target_seq = seq_arr\n",
    "                else:\n",
    "                   target_seq = np.concatenate((target_seq, seq_arr), axis=0, dtype=np.uint8)\n",
    "                del all_seq\n",
    "\n",
    "                # Target index building\n",
    "                sequence_len = len(seq_arr)\n",
    "                mask = generate_mask(kmer_len)\n",
    "\n",
    "                # Function to compute kmer value based on the previous (on the left side) kmer value and new nucleotide\n",
    "                uadd = np.frompyfunc(lambda x, y: ((x << 2) | y) & mask, 2, 1)\n",
    "\n",
    "                # This computes values for kmers\n",
    "                kmers = uadd.accumulate(seq_arr, dtype=object).astype(int)\n",
    "                kmers[:kmer_len-2] = 0\n",
    "                del seq_arr\n",
    "                \n",
    "                # Do sliding window and get min kmers positions\n",
    "                kmers_min_pos = np.add(np.argmin(sliding_window_view(kmers, window_shape=window_len), axis=1), np.arange(0, sequence_len - window_len + 1))\n",
    "                \n",
    "                # Now collect all selected mimumum and kmers into single table\n",
    "                selected_kmers = np.column_stack((\n",
    "                    kmers[kmers_min_pos],\n",
    "                    kmers_min_pos,\n",
    "                    #np.ones(len(kmers_min_pos), dtype=bool)\n",
    "                ))[kmer_len:]\n",
    "                del kmers_min_pos\n",
    "                del kmers\n",
    "                gc.collect()\n",
    "\n",
    "                # Remove duplicates\n",
    "                selected_kmers = selected_kmers[selected_kmers[:, 0].argsort()]\n",
    "                selected_kmers = np.unique(selected_kmers, axis=0)\n",
    "\n",
    "                # Shift all indices according to what we loaded already\n",
    "                selected_kmers[:,1] += index_offset\n",
    "\n",
    "                # This part performs group by using the kmer value\n",
    "                selected_kmers_unique_idx = np.unique(selected_kmers[:, 0], return_index=True)[1][1:]\n",
    "                selected_kmers_entries_split = np.split(selected_kmers[:, 1], selected_kmers_unique_idx)\n",
    "\n",
    "                if len(selected_kmers) > 0:\n",
    "                    # We zip all kmers into a dict\n",
    "                    i = 0\n",
    "                    for k, v in zip(chain([selected_kmers[0, 0]], selected_kmers[selected_kmers_unique_idx, 0]), selected_kmers_entries_split):\n",
    "                        i += 1\n",
    "                        # TODO: REMOVE SOME FROM INDEX\n",
    "                        if i >= 20 and len(v) == 1:\n",
    "                            i = 0\n",
    "                            continue\n",
    "                        if k in ref_index:\n",
    "                            ref_index[k] = np.concatenate((ref_index[k], v), axis=0)\n",
    "                        else:\n",
    "                            ref_index[k] = v\n",
    "                else:\n",
    "                    # If we have no minimizers we return nothing, sorry\n",
    "                    pass\n",
    "\n",
    "                index_offset += all_seq_len\n",
    "                all_seq_len = 0\n",
    "                all_seq = \"\"\n",
    "                print(f\"PROCESSED ENTIRE CHUNK! offset={index_offset}\")\n",
    "                del selected_kmers_unique_idx\n",
    "                del selected_kmers_entries_split\n",
    "                gc.collect()\n",
    "            if line[0] == '>':\n",
    "                if ref_loaded:\n",
    "                    break\n",
    "                ref_loaded = True\n",
    "                continue\n",
    "\n",
    "    gc_collect_cnt = 300\n",
    "    output_buf = []\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        query_id = \"\"\n",
    "        query_seq = \"\"\n",
    "        with open(reads_file_path) as reads_fh:\n",
    "            for line in chain(reads_fh, [\">\"]):\n",
    "                if line[0] == '>' and len(query_seq) > 0:\n",
    "                    query_seq = MAPPING_FN(np.array(list(query_seq)))\n",
    "                    # Process\n",
    "                    if gc_collect_cnt > 299:\n",
    "                       gc_collect_cnt = 0\n",
    "                       gc.collect()\n",
    "                    gc_collect_cnt += 1\n",
    "                    \n",
    "                    # if query_id not in ['read_937', 'read_961', 'read_972', 'read_96', 'read_126', 'read_394', 'read_561', 'read_693', 'read_771', 'read_794', 'read_817', 'read_903', 'read_910', 'read_937', 'read_972', 'read_961']:\n",
    "                    #    continue\n",
    "                    if query_id == 'read_25': #int(query_id.split('_')[1]) < 100 or query_id in ['read_937', 'read_961', 'read_972', 'read_96', 'read_126', 'read_394', 'read_561', 'read_693', 'read_771', 'read_794', 'read_817', 'read_903', 'read_910', 'read_937', 'read_972', 'read_961']:\n",
    "                        try:\n",
    "                            max_diff = round(len(query_seq)*1.3)\n",
    "                            min_index_query = get_minimizers(\n",
    "                                query_seq,\n",
    "                                kmer_len=kmer_len,\n",
    "                                window_len=window_len,\n",
    "                            )\n",
    "\n",
    "                            common_kmers = []\n",
    "                            for key in min_index_query:\n",
    "                                if key in ref_index:\n",
    "                                    common_kmers.append(key)\n",
    "\n",
    "                            matches = np.array([[-1, -1]])\n",
    "                            for kmer in common_kmers:\n",
    "                                kmer_entries_target, kmer_entries_query = ref_index[kmer], min_index_query[kmer]\n",
    "                                matches = np.concatenate((\n",
    "                                    matches,\n",
    "                                    cartesian_product(\n",
    "                                        kmer_entries_target,\n",
    "                                        kmer_entries_query,\n",
    "                                    )),\n",
    "                                    axis=0,\n",
    "                                )\n",
    "                            matches = matches[matches[:, 0].argsort()]\n",
    "                            matches = matches[1:]\n",
    "                            n = len(matches)\n",
    "                            \n",
    "                            match_score, match_start_t, match_end_t, match_start_q, match_end_q = -max_diff, 0, 0, 0, 0\n",
    "\n",
    "                            # print(\"ALL MATCH:\")\n",
    "                            # print(matches)\n",
    "                            # print(\"END\")\n",
    "\n",
    "                            if n == 0:\n",
    "                                pass\n",
    "                            elif n == 1:\n",
    "                                match_score, match_start_t, match_end_t, match_start_q, match_end_q = 0, matches[0, 0], matches[0, 0], matches[0, 1], matches[0, 1]\n",
    "                            else:\n",
    "                                longest_seq_len = 0\n",
    "                                parent = [999999999]*(n+1)\n",
    "                                increasingSub = [999999999]*(n+1)\n",
    "                                for i in range(n):\n",
    "                                    start = 1\n",
    "                                    end = longest_seq_len\n",
    "                                    while start <= end:\n",
    "                                        middle = (start + end) // 2\n",
    "                                        if matches[increasingSub[middle], 1] < matches[i, 1]:\n",
    "                                            start = middle + 1\n",
    "                                        else:\n",
    "                                            end = middle - 1\n",
    "                                    parent[i] = increasingSub[start-1]\n",
    "                                    increasingSub[start] = i\n",
    "\n",
    "                                    if start > longest_seq_len:\n",
    "                                        longest_seq_len = start\n",
    "\n",
    "                                current_node = increasingSub[longest_seq_len]\n",
    "                                q = [current_node]*longest_seq_len \n",
    "                                for j in range(longest_seq_len-1, 0, -1):\n",
    "                                    current_node = parent[current_node]\n",
    "                                    q[j-1] = current_node\n",
    "\n",
    "                                lis = np.take(matches, q, axis=0)\n",
    "                                for i in range(longest_seq_len):\n",
    "                                    start = i\n",
    "                                    end = longest_seq_len\n",
    "                                    while start <= end:\n",
    "                                        middle = (start + end) // 2\n",
    "                                        if middle == longest_seq_len:\n",
    "                                            start = longest_seq_len\n",
    "                                            break\n",
    "                                        if lis[middle, 0] < lis[i, 0] + max_diff - lis[i, 1]:\n",
    "                                            start = middle + 1\n",
    "                                        else:\n",
    "                                            end = middle - 1\n",
    "                                    # Window is i till end\n",
    "                                    # print(f\"Start from {i} (till {start} whcih has value\") #[{lis[start, 0]}, {lis[start, 1]}])\n",
    "                                    estimated_matches_q = (lis[start, 1] if start < longest_seq_len else max_diff) - lis[i, 1]\n",
    "                                    estimated_matches_t = (lis[start, 0] if start < longest_seq_len else lis[start-1, 0]) - lis[i, 0]\n",
    "                                    score = min(estimated_matches_q, estimated_matches_t)*min(estimated_matches_q, estimated_matches_t) - np.sum(np.diff(lis[i:start, 0], axis=0))\n",
    "                                    # print(lis[i:start])\n",
    "                                    # print(f\"LAST ELEMENT IS {lis[i:start][-1]} where start={start} and l-1={longest_seq_len-1}\")\n",
    "                                    # print(f\"score = {score}\")\n",
    "                                    if score > match_score:\n",
    "                                        match_end_index_pos = max(i, min(start-1, longest_seq_len-1))\n",
    "                                        match_score, match_start_t, match_end_t, match_start_q, match_end_q = score, lis[i, 0], lis[match_end_index_pos, 0], lis[i, 1], lis[match_end_index_pos, 1]\n",
    "                                        #print(f\"ACCEPTED SCORE: {match_start_t} - {match_end_t}\")\n",
    "                                    if start == longest_seq_len:\n",
    "                                        break\n",
    "\n",
    "                            #print(f\"SCORE: Match score is {match_score}\")\n",
    "                            #print(f\"SCORE: Match around {match_start_t} - {match_end_t}\")\n",
    "                            #sys.exit(1)\n",
    "\n",
    "                            # q_begin, q_end, t_begin, t_end, list_length\n",
    "\n",
    "                            relative_extension = kmer_len // 2 + 1\n",
    "\n",
    "                            if abs(match_end_t - match_start_t) > max_diff + relative_extension:\n",
    "                                # FAILED MAPPING!\n",
    "                                #print(f\"Failed sequence, reason: {match_start_t} - {match_end_t} ({abs(match_end_t - match_start_t)})\")\n",
    "                                output_buf.append(f\"{query_id} status=FAIL\\n\")\n",
    "                            else:\n",
    "                                q_begin, q_end = 0, len(query_seq)\n",
    "                                t_begin, t_end = match_start_t - match_start_q - relative_extension, match_end_t + (len(query_seq)-match_end_q) + relative_extension\n",
    "\n",
    "                                q_begin, q_end = normalize_pos(q_begin, len(query_seq)), normalize_pos(q_end, len(query_seq))\n",
    "                                t_begin, t_end = normalize_pos(t_begin, len(target_seq)), normalize_pos(t_end, len(target_seq))\n",
    "\n",
    "                                realign_mode = 0\n",
    "                                # t_begin_pad, t_end_pad, should_realign_right = run_match_align_bwt(\n",
    "                                #     query_seq,\n",
    "                                #     target_seq[t_begin:t_end],\n",
    "                                # )\n",
    "                                GLO_Q = query_seq\n",
    "                                GLO_T = target_seq[t_begin:t_end]\n",
    "                                return None\n",
    "                                    \n",
    "                                if should_realign_right:\n",
    "                                    realign_mode = 1\n",
    "                                if abs(t_end-(t_end_pad or 0)-t_begin-(t_begin_pad or 0)) > len(query_seq)*1.05:\n",
    "                                    realign_mode = 2\n",
    "                                    if t_begin_pad is not None:\n",
    "                                        t_begin += t_begin_pad\n",
    "                                    if t_end_pad is not None:\n",
    "                                        t_end -= t_end_pad\n",
    "\n",
    "                                if realign_mode > 0:\n",
    "                                    t_begin_pad, t_end_pad = run_match_align_dp(\n",
    "                                        target_seq[t_begin:t_end],\n",
    "                                        query_seq,\n",
    "                                        align_mode=realign_mode,\n",
    "                                    )\n",
    "\n",
    "                                if not should_realign_right and t_begin_pad is None:\n",
    "                                    t_begin_pad = relative_extension\n",
    "                                if not should_realign_right and t_end_pad is None:\n",
    "                                    t_end_pad = relative_extension\n",
    "\n",
    "                                if t_begin_pad is not None:\n",
    "                                    t_begin += t_begin_pad\n",
    "                                if t_end_pad is not None:\n",
    "                                    t_end -= t_end_pad\n",
    "\n",
    "                                # print(\"TARGET!!!!\")\n",
    "                                # print(\"\".join([RR_MAPPING[i] for i in target_seq[t_begin:t_end].tolist()]))\n",
    "                                # print(\"QUERY!!!\")\n",
    "                                # print(\"\".join([RR_MAPPING[i] for i in query_seq.tolist()]))\n",
    "                                #print(f\"ALIGNED: {t_begin} - {t_end} (pd: {t_begin_pad}, {t_end_pad} query: {q_begin} - {q_end})\")\n",
    "                                # sys.exit(1)\n",
    "\n",
    "                                # print(\"TARGET!!!!\")\n",
    "                                # print(\"\".join([RR_MAPPING[i] for i in target_seq[t_begin:t_end].tolist()]))\n",
    "                                # print(\"QUERY!!!\")\n",
    "                                # print(\"\".join([RR_MAPPING[i] for i in query_seq.tolist()]))\n",
    "\n",
    "                                #est_edit_dist = estimate_distance(target_seq[t_begin:t_end], query_seq) #levenshtein(\"\".join([RR_MAPPING[i] for i in target_seq[t_begin:t_end].tolist()]), \"\".join([RR_MAPPING[i] for i in query_seq.tolist()]))\n",
    "                                # est_edit_dist = levenshtein(\n",
    "                                #    \"\".join([RR_MAPPING[i] for i in target_seq[t_begin:t_end].tolist()]),\n",
    "                                #    \"\".join([RR_MAPPING[i] for i in query_seq.tolist()]),\n",
    "                                #    177, 2, 2, 1\n",
    "                                # )\n",
    "\n",
    "                                if query_id in expected_coords:\n",
    "                                   diff_start = expected_coords[query_id][0]-t_begin\n",
    "                                   diff_end = expected_coords[query_id][1]-t_end\n",
    "                                   #print(f\"TOTAL DIFF: {max(abs(diff_start), abs(diff_end))}\")\n",
    "                                   status = \"OK\" if max(abs(diff_start), abs(diff_end)) < 20 else \"BAD\"\n",
    "                                   qual = \"AA\" if abs(diff_start)+abs(diff_end) < 10 else (\"AB\" if abs(diff_start)+abs(diff_end) < 20 else (\"C\" if max(abs(diff_start), abs(diff_end)) < 20 else \"D\"))\n",
    "                                   #output_buf.append\n",
    "                                   output_file.write(f\"{query_id} status={status} qual={qual} diff=<{diff_start}, {diff_end}>  | {t_begin} {t_end} | pad: {t_begin_pad}, {t_end_pad} | {'REALIGNED'+str(realign_mode) if realign_mode != 0 else ''} \\n\")\n",
    "                                else:\n",
    "                                    output_buf.append(f\"{query_id} {t_begin} {t_end}\\n\")\n",
    "                        except Exception as e:\n",
    "                            # TODO?\n",
    "                            print(e)\n",
    "                            raise e\n",
    "                if line[0] == '>':\n",
    "                    # Process end\n",
    "                    query_seq = \"\"\n",
    "                    query_id = line[1:].strip()\n",
    "                else:\n",
    "                    query_seq += line.rstrip()\n",
    "        if len(output_buf) > 0:\n",
    "            output_file.writelines(output_buf)\n",
    "        print(f\"Wrote records to {output_file_path}\")\n",
    "        #os._exit(0) # Faster exit than normally\n",
    "\n",
    "run_aligner_pipeline(\n",
    "    reference_file_path=\"../data_big/reference20M.fasta\",\n",
    "    reads_file_path=\"../data_big/reads20Mb.fasta\",\n",
    "    output_file_path=\"output.txt\",\n",
    "    kmer_len=16,\n",
    "    window_len=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4bf2b00e-f40f-45e8-a913-3363b6d37960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999620"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GLO_CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c838117c-d81b-4302-b7ba-f1c0045b8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used to create kmer binary masks\n",
    "MAX_KMER_SIZE = 64\n",
    "\n",
    "# Mapping of special character used by suffix tables and similar structures\n",
    "MAPPING_DOLLAR = 0\n",
    "# Mapping of nucleotides\n",
    "MAPPING = dict(\n",
    "    C=1,\n",
    "    A=2,\n",
    "    T=3,\n",
    "    G=4,\n",
    ")\n",
    "# Allphabet\n",
    "ALPHABET = set(MAPPING.values())\n",
    "# Alphabet with included special character\n",
    "ALPHABET_DOLLAR = set([*ALPHABET, MAPPING_DOLLAR])\n",
    "\n",
    "# Pairs of alphabet symbols (a, b) such that a > b\n",
    "# Useful for O(a^2) iteration over alphabet\n",
    "ALPHABET_LT_PAIRS = [(a, b) for a in ALPHABET for b in ALPHABET if b < a]\n",
    "\n",
    "# Vectorized numpy function to map nucleotides from string\n",
    "MAPPING_FN = vectorize(MAPPING.get, otypes=[uint8])\n",
    "\n",
    "# Remove every nth kmer from the reference index\n",
    "REFERENCE_NTH_KMER_REMOVAL = 15 # was: 20\n",
    "# Load reference in chunks of that size\n",
    "REFERENCE_INDEX_CHUNK_SIZE = 1000000\n",
    "\n",
    "# Length of k-mer used to generate (k,w)-minimizer indices\n",
    "KMER_LEN = 16 # was: 16\n",
    "# Length of window to generate (k,w)-minimizer indices\n",
    "WINDOW_LEN = 8 # was: 5\n",
    "\n",
    "# Match from k-mer index with be resized by kmer_len times this factor\n",
    "FACT_KMER_TO_RELATIVE_EXTENSION_LEN = 0.5\n",
    "\n",
    "# Miminal length of subsequent k-mers that form a valid match\n",
    "MIN_LIS_EXTENSION_WINDOW_LEN = 3\n",
    "# Max distance between starting k-mer of the match and the ending k-mer of match\n",
    "# Used when filtering the extended seed\n",
    "# Should be higher than FACT_TARGET_TO_QUERY_MAX_RELATIVE_LENGTH\n",
    "FACT_LIS_MAX_QUERY_DISTANCE = 1.3\n",
    "\n",
    "# Max difference between query and target to concude that it's a valid match\n",
    "#\n",
    "# if len(t_region) > len(q_region) * FACT_TARGET_TO_QUERY_MAX_RELATIVE_LENGTH:\n",
    "#   invalid_match()\n",
    "#\n",
    "FACT_TARGET_TO_QUERY_MAX_RELATIVE_LENGTH = 1.05\n",
    "# When using BWT aligner this is the maximum extra padding that we consider valid (as a fraction of len(query))\n",
    "FACT_BWT_QUERY_MAX_OFFSET = 0.04\n",
    "# When using BWT aligner this is the length of query part that we consider for fast matching (as a fraction of len(query))\n",
    "FACT_BWT_QUERY_FRAGMENT_SIZE = 0.1\n",
    "# When using BWT aligner this is the maximum number of error that we should encounter (to speed up searching)\n",
    "# The threshold is calculated as follows:\n",
    "#\n",
    "#    fragment_len = len(query) * FACT_BWT_QUERY_FRAGMENT_SIZE\n",
    "#    max_errors = fragment_len * FACT_BWT_FRAGMENT_REL_ERRORS_THRESHOLD\n",
    "#\n",
    "FACT_BWT_FRAGMENT_REL_ERRORS_THRESHOLD = 0.08\n",
    "\n",
    "# When using DP aligner (more accurate than BWT, but also slower)\n",
    "# we consider pairs (kmer_len, kmer_skip)\n",
    "# kmer_len is the length of k-mer we consider\n",
    "# kmer_skip means we skip every n-th kmer \n",
    "# Those values can be significantlly lower than global KMER_SIZE, because we run DP aligner only in specific situations\n",
    "# If no match is found we use the next configuration untill we find anything\n",
    "DP_K_STEP_SEQUENCE = [(15, 11), (10, 11), (8, 5)]\n",
    "# Length of the query suffix we use for the DP aligner as a fraction of query length\n",
    "FACT_DP_QUERY_SUFFIX_REL_LEN = 0.4\n",
    "# For DP aligner we set maxiumum exit distance \n",
    "# This value is fraction of the length of the query\n",
    "FACT_DP_QUERY_REL_MAX_E_DISTANCE = 0.1111\n",
    "\n",
    "# Cost when gap is opened (DP aligner uses only COST_GAP_EXTEND for gaps)\n",
    "COST_GAP_OPEN = 3\n",
    "# Cost when gap is extended (DP aligner uses only COST_GAP_EXTEND for gaps)\n",
    "COST_GAP_EXTEND = 1\n",
    "# Cost of mismatch\n",
    "COST_MISMATCH = 1\n",
    "# Cost of match\n",
    "COST_MATCH = 0\n",
    "\n",
    "# Globals used to cache values and speed up calculations (no need to pass around many references in call frames)\n",
    "# C[a] := number of lexicographically smaller letters than a in bw/reference\n",
    "_global_bwt_c = {}\n",
    "# O is a dictionary with keys $,A,C,G,T, and values are arrays of counts\n",
    "_global_bwt_o = {}\n",
    "# D[i] := lower bound on number of differences in substring s[1:i]\n",
    "_global_bwt_d = []\n",
    "# mask[k] := mask used to calculate hash for k-mer of length k\n",
    "_global_masks: Optional[Dict[int, int]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8c5bd30f-1c6d-425f-9e82-01a6e2332950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg[samples=1]: 822.0 ms\n"
     ]
    }
   ],
   "source": [
    "def gc_collect():\n",
    "    pass\n",
    "\n",
    "def test_ib000():\n",
    "    global GLO_CHUNK\n",
    "    seq_arr = GLO_CHUNK\n",
    "    index_offset = 1000000\n",
    "    ref_index = dict()\n",
    "\n",
    "    # Target index building\n",
    "    sequence_len = len(seq_arr)\n",
    "    mask = generate_mask(KMER_LEN)\n",
    "\n",
    "    # Function to compute kmer value based on the previous (on the left side) kmer value and new nucleotide\n",
    "    uadd = frompyfunc(lambda x, y: ((x << 2) | y) & mask, 2, 1)\n",
    "\n",
    "    # This computes values for kmers\n",
    "    kmers = uadd.accumulate(seq_arr, dtype=object).astype(int)\n",
    "    kmers[:KMER_LEN-2] = 0\n",
    "    del seq_arr\n",
    "    \n",
    "    # Do sliding window and get min kmers positions\n",
    "    kmers_min_pos = add(argmin(sliding_window_view(kmers, window_shape=WINDOW_LEN), axis=1), arange(0, sequence_len - WINDOW_LEN + 1))\n",
    "    \n",
    "    # Now collect all selected mimumum and kmers into single table\n",
    "    selected_kmers = column_stack((\n",
    "        kmers[kmers_min_pos],\n",
    "        kmers_min_pos,\n",
    "        #np.ones(len(kmers_min_pos), dtype=bool)\n",
    "    ))[KMER_LEN:]\n",
    "    del kmers_min_pos\n",
    "    del kmers\n",
    "    gc_collect()\n",
    "\n",
    "    # Remove duplicates\n",
    "    selected_kmers = selected_kmers[selected_kmers[:, 0].argsort()]\n",
    "    selected_kmers = unique(selected_kmers, axis=0)\n",
    "\n",
    "    # Shift all indices according to what we loaded already\n",
    "    selected_kmers[:,1] += index_offset\n",
    "\n",
    "    # This part performs group by using the kmer value\n",
    "    selected_kmers_unique_idx = unique(selected_kmers[:, 0], return_index=True)[1][1:]\n",
    "    selected_kmers_entries_split = split(selected_kmers[:, 1], selected_kmers_unique_idx)\n",
    "\n",
    "    if len(selected_kmers) > 0:\n",
    "        # We zip all kmers into a dict\n",
    "        i = 0\n",
    "        for k, v in zip(chain([selected_kmers[0, 0]], selected_kmers[selected_kmers_unique_idx, 0]), selected_kmers_entries_split):\n",
    "            i += 1\n",
    "            # Remove every n-th kmer from index\n",
    "            if i >= REFERENCE_NTH_KMER_REMOVAL and len(v) == 1:\n",
    "                i = 0\n",
    "                continue\n",
    "            # This part merges index for a chunk into index for the entire reference sequence\n",
    "            # this way (by loading in chunks) we spare some memory\n",
    "            if k in ref_index:\n",
    "                ref_index[k] = concatenate((ref_index[k], v), axis=0)\n",
    "            else:\n",
    "                ref_index[k] = v\n",
    "    else:\n",
    "        # If we have no minimizers we return nothing, sorry\n",
    "        pass\n",
    "    return ref_index\n",
    "\n",
    "def test_ib():\n",
    "    global GLO_CHUNK\n",
    "    seq_arr = GLO_CHUNK\n",
    "    index_offset = 1000000\n",
    "    ref_index = dict()\n",
    "\n",
    "    # Target index building\n",
    "    sequence_len = len(seq_arr)\n",
    "    mask = generate_mask(KMER_LEN)\n",
    "\n",
    "    # Function to compute kmer value based on the previous (on the left side) kmer value and new nucleotide\n",
    "    uadd = frompyfunc(lambda x, y: ((x << 2) | y) & mask, 2, 1)\n",
    "\n",
    "    # This computes values for kmers\n",
    "    kmers = uadd.accumulate(seq_arr, dtype=object).astype(int)\n",
    "    kmers[:KMER_LEN-2] = 0\n",
    "    del seq_arr\n",
    "    \n",
    "    # Do sliding window and get min kmers positions\n",
    "    kmers_min_pos = add(argmin(sliding_window_view(kmers, window_shape=WINDOW_LEN), axis=1), arange(0, sequence_len - WINDOW_LEN + 1))\n",
    "    \n",
    "    # Now collect all selected mimumum and kmers into single table\n",
    "    selected_kmers = column_stack((\n",
    "        kmers[kmers_min_pos],\n",
    "        kmers_min_pos + index_offset,\n",
    "        #np.ones(len(kmers_min_pos), dtype=bool)\n",
    "    ))[KMER_LEN:]\n",
    "    del kmers_min_pos\n",
    "    del kmers\n",
    "    gc_collect()\n",
    "\n",
    "    # Remove duplicates\n",
    "    selected_kmers = selected_kmers[selected_kmers[:, 0].argsort()]\n",
    "    selected_kmers = unique(selected_kmers, axis=0)\n",
    "\n",
    "    # Shift all indices according to what we loaded already\n",
    "    #selected_kmers[:,1] += index_offset\n",
    "\n",
    "    # This part performs group by using the kmer value\n",
    "    selected_kmers_unique_idx = unique(selected_kmers[:, 0], return_index=True)[1][1:]\n",
    "    selected_kmers_entries_split = split(selected_kmers[:, 1], selected_kmers_unique_idx)\n",
    "\n",
    "    if len(selected_kmers) > 0:\n",
    "        # We zip all kmers into a dict\n",
    "        i = 0\n",
    "        for k, v in zip(chain([selected_kmers[0, 0]], selected_kmers[selected_kmers_unique_idx, 0]), selected_kmers_entries_split):\n",
    "            i += 1\n",
    "            # Remove every n-th kmer from index\n",
    "            if i >= REFERENCE_NTH_KMER_REMOVAL and len(v) == 1:\n",
    "                i = 0\n",
    "                continue\n",
    "            # This part merges index for a chunk into index for the entire reference sequence\n",
    "            # this way (by loading in chunks) we spare some memory\n",
    "            if k in ref_index:\n",
    "                ref_index[k] = concatenate((ref_index[k], v), axis=0)\n",
    "            else:\n",
    "                ref_index[k] = v\n",
    "    else:\n",
    "        # If we have no minimizers we return nothing, sorry\n",
    "        pass\n",
    "    return ref_index\n",
    "\n",
    "def test_ib_run():\n",
    "    samples = 1\n",
    "    from time import time_ns\n",
    "    times = []\n",
    "    for i in range(samples):\n",
    "        start = time_ns()\n",
    "        test_ib()\n",
    "        end = time_ns()\n",
    "        times.append(end-start)\n",
    "    print(f\"Avg[samples={samples}]: {(sum(times) / len(times)) // 1000000} ms\")\n",
    "\n",
    "test_ib_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5e554-72fb-419a-9455-1ac556153b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
